"""
This script discovers optimum per-exercise challenge card recommendation
thresholds by maximizing F-scores for various subscripts. These thresholds are
then injected into the pickled knowledge model so that the implementation of
the knowledge model has access to them to make recommendations based on
whether the predicted value is greater than the optimized threshold.

Previously, challenge cards were awarded for exercises that UserKnowledge
predicts success with > 85%% liklihood. This 85%% was global, regardless
of the exercise that was being awarded as a challenge card.

Now, we have thresholds on a per-exercise basis. We discovered the best
thresholds for each exercise by optimizing various F scores over a set of
training data. F score is a way to balance precision and recall. F1 weights
precision and recall equally. F0.5 puts twice the weight on precision. F2
puts twice the weight on recall. We computed F scores per-exercise for
subscripts 0.1, 0.25, 0.5, 1.0, 2.0, and 2.5. In general, the higher the
subscript, the lower the threshold, since a threshold of 0%% will always
have a recall of 100%%.

The training data should be in the format generated by
accuracy_model_featureset.py. The top-level docstring in that file will guide
you through the process of collecting and formatting your training and test
data sets.

An example invocation of this script:
python optimize_exercise_thresholds.py --samples features3.head3.sorted.csv \
    --knowledge_model new_model3.grid.pkl 0.1 0.25 0.5 1 2 2.5

You can then view the thresholds that were discovered with:
python dump_knowledge_model_metadata.py -f new_model3.grid.pkl
"""
import argparse
import pickle
import sys

import numpy as np
import scipy.optimize

from accuracy_model_train import FeaturesetFields
from accuracy_model_train import parse_features
import accuracy_model_util
import regression_util

idx = FeaturesetFields()


def f_score(threshold, subscript, labels, predictions):
    """
    http://en.wikipedia.org/wiki/F1_score
    """
    # TODO(mattfaus): Would this vectorized implementation work and be faster?
    # true_positives = labels & (predictions > thresholds)
    # tpr = float(true_positives.sum()) / labels.size

    true_positives = len([label for label, prediction
        in zip(labels, predictions) if label and prediction > threshold])
    tpr = float(true_positives) / len(labels)

    false_positives = len([label for label, prediction
        in zip(labels, predictions) if not label and prediction > threshold])
    fpr = float(false_positives) / len(labels)

    false_negatives = len([label for label, prediction
        in zip(labels, predictions) if label and prediction <= threshold])
    fnr = float(false_negatives) / len(labels)

    f_score = (((1 + subscript ** 2) * tpr /
        ((1 + subscript ** 2) * tpr + subscript ** 2 * fnr + fpr)))

    # The optimization function is trying to minimize, so negate it
    return -f_score


def find_optimum_thresholds(search_method, subscripts, knowledge_model,
    samples):
    """Finds the threshold which will optimize a certain F-score for a single
    exercise.

    Arguments:
        search_method - The search method to find the optimum f-scores, can be
            brute or minimize_scalar.
        subscripts - A list of floats specifying the f-score subscripts to find
            optimum thresholds fore.
        knowledge_model - The knowledge model which contains the theta values
            used to compute predictions from the samples.
        samples - A list of samples as output by accuracy_model_featureset.py.

    Returns:
        A dict like this: {
            subscripts[0]: {
                "max_score": 0.83,
                "success": True,
                "threshold": 0.65,
                "samples": 23108,
            }
            subscripts[1]: ...
            subscripts[n]: ...
        }
    """
    if len(samples) == 0:
        return None

    exercise_name = samples[0][idx.exercise]
    if exercise_name not in knowledge_model["thetas"]:
        return None
    else:
        thetas = knowledge_model["thetas"][exercise_name]

    # Convert CSV features into proper float-array representation
    correct, features, _ = parse_features(samples, False, ["custom", "random"])

    # Compute predctions based on features
    predictions = regression_util.sigmoid(np.dot(features, thetas))

    thresholds = {}
    for subscript in subscripts:
        if search_method.lower() == "brute":
            optimum_threshold = scipy.optimize.brute(f_score,
                ranges=((0.0, 1.0),), Ns=101, args=(subscript, correct,
                    predictions),
                full_output=True)

            # Transform to standard format
            optimum_threshold = {
                "max_score": -optimum_threshold[1],
                "success": True,
                "threshold": optimum_threshold[0][0],
            }

        elif search_method.lower() == "minimize_scalar":
            optimum_threshold = scipy.optimize.minimize_scalar(f_score,
                method="bounded", bounds=(0.0, 1.0), args=(subscript,
                    correct, predictions))

            # Transform to standard format
            optimum_threshold = {
                "max_score": -optimum_threshold.fun,
                "success": optimum_threshold.success,
                "threshold": optimum_threshold.x,
            }

        else:
            raise ValueError("Did not understand search method %s" %
                search_method)

        if not optimum_threshold["success"]:
            print >>sys.stderr, "Optimization failed for", subscript

        # Augment the result object with the number of samples
        optimum_threshold["samples"] = len(samples)

        thresholds[subscript] = optimum_threshold
    return thresholds


def parse_command_line():
    parser = argparse.ArgumentParser(
        description=("Find optimum recommendation thresholds for each "
            "exercise in a knowledge model params file, based on maximizing "
            "their F-scores for a set of training data."))

    parser.add_argument("--samples", default=None,
        help=("The CSV feature samples, as output by "
            "accuracy_model_featureset.py"))

    parser.add_argument("--knowledge_model", default=None,
        help=("The pickled knowledge model file. Theta values will be read "
            "from and optimum thresholds will be written into this file."))

    parser.add_argument("--search_method", default="brute",
        help=("The technique used to find optimum f-scores. Valid values are "
            "brute or minimize_scalar"))

    parser.add_argument("subscripts", type=float, nargs="+",
        help="The subscripts of the F scores to optimize for.")

    args = parser.parse_args()

    # Read the knowledge model file and do some validation
    args.knowledge_model_filepath = args.knowledge_model
    with open(args.knowledge_model, "r") as km_file:
        args.knowledge_model = pickle.load(km_file)

    assert "thetas" in args.knowledge_model

    return args


def main():
    args = parse_command_line()

    prev_exercise = None
    samples = []
    exercise_thresholds = {}

    with open(args.samples, "r") as sample_file:
        for line in sample_file:
            row = accuracy_model_util.linesplit.split(line.strip())
            exercise_name = row[idx.exercise]
            problem_number = int(row[idx.problem_number])

            # If we've consumed all of an exercise's samples, find the optimums
            if prev_exercise and exercise_name != prev_exercise:
                print "Finding optimum thresholds for", exercise_name
                thresholds = find_optimum_thresholds(args.search_method,
                    args.subscripts, args.knowledge_model, samples)
                exercise_thresholds[exercise_name] = thresholds
                samples = []

            prev_exercise = exercise_name

            # Find the optimum thresholds for first problem attempts only
            # TODO(mattfaus): Move this into a command-line argument
            if problem_number == 1:
                samples.append(row)

    thresholds = find_optimum_thresholds(args.search_method, args.subscripts,
        args.knowledge_model, samples)
    exercise_thresholds[exercise_name] = thresholds

    args.knowledge_model["f_thresholds"] = {}

    for exercise, scores in exercise_thresholds.items():
        if scores == None:
            print "No thresholds found for", exercise
            continue

        # Inject to knowledge_model
        args.knowledge_model["f_thresholds"][exercise] = scores

    # Write out new knowledge model
    print "Writing optimum thresholds to", args.knowledge_model_filepath
    with open(args.knowledge_model_filepath, "w") as km_file:
        pickle.dump(args.knowledge_model, km_file)

    # Note: You can use dump_knowledge_model_metadata.py to inspect

if __name__ == '__main__':
    main()
